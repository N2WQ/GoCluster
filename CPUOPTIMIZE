# DXCluster CPU + Latency Optimization Notes (Full Findings)
Date: 2025-12-14
Updated: 2025-12-15 (fresh headless 15m CPU/heap profiles + reprioritized remaining work)

This file captures the full repo review, hot-path analysis, and prioritized recommendations so we do not need to redo the work later.

---

## Update (2025-12-15): Fresh CPU/Heap Profiles + Remaining Optimization Opportunities

This update re-runs profiling against current code + current data and captures the full results under `data/diagnostics/`.
It also answers the “is telemetry still wired?” and “do we have the right profiling setup?” questions with evidence.

### Telemetry / profiling wiring (confirmed)
- `main.go:maybeStartDiagServer` exposes `/debug/pprof/*` and `/debug/heapdump` when `DXC_PPROF_ADDR` is set.
- `main.go:maybeStartHeapLogger` periodically prints heap stats when `DXC_HEAP_LOG_INTERVAL` is set.

Note (Windows): in this run, the diag server bound on IPv4; `Invoke-WebRequest http://localhost:PORT/...` can fail if `localhost` resolves to IPv6 first. Use `127.0.0.1` in the capture URLs, or set `DXC_PPROF_ADDR=127.0.0.1:6061`.

### Profiling run details (this update)
- Build: `go build -o .\gocluster_profile.exe .`
- Headless mode: `config.yaml` has `ui.mode: "headless"` (to avoid UI overhead during profiling).
- Diag server enabled via env: `DXC_PPROF_ADDR=localhost:6061` (then profiled via `http://127.0.0.1:6061/...`).
- Workload: all sources enabled (`rbn`, `rbn_digital`, `human_telnet`, `pskreporter`); **0 telnet clients** (fan-out not stressed).
- Session ID: `20251215-195000` (used in filenames below).

Captured artifacts:
- CPU profile (15m): `data/diagnostics/cpu-15m-20251215-195000.pprof`
- Heap (start/end, GC forced): `data/diagnostics/heap-start-20251215-195000.pprof`, `data/diagnostics/heap-end-20251215-195000.pprof`
- Session metadata + logs: `data/diagnostics/profile-session-20251215-195000.json`, `data/diagnostics/run-20251215-195000.out.log`, `data/diagnostics/run-20251215-195000.err.log`
- Derived pprof text outputs (saved so we don’t need to rerun commands):
  - CPU: `data/diagnostics/pprof-cpu-top100-20251215-195000.txt`, `data/diagnostics/pprof-cpu-cum100-20251215-195000.txt`
  - CPU line-level: `data/diagnostics/pprof-cpu-list-processOutputSpots-20251215-195000.txt`, `data/diagnostics/pprof-cpu-list-gridCache-shouldUpdate-20251215-195000.txt`, `data/diagnostics/pprof-cpu-list-gridstore-Get-20251215-195000.txt`
  - Heap: `data/diagnostics/pprof-heap-inuse-20251215-195000.txt`, `data/diagnostics/pprof-heap-alloc-20251215-195000.txt`, `data/diagnostics/pprof-heap-alloc-cum-20251215-195000.txt`, `data/diagnostics/pprof-heap-diff-inuse-20251215-195000.txt`
  - Alloc line-level (smoking gun): `data/diagnostics/pprof-alloc-list-distanceCache-configure-20251215-195000.txt`

### Key results (2025-12-15 profiles)

#### CPU (15m)
- CPU total: `55.74s` samples over `900s` → average ~`0.062` cores (~`6.2%` of one core) for this workload.
- Cumulative hotspots (application-level):
  - `main.processOutputSpots` cum `~11.30s` (`data/diagnostics/pprof-cpu-cum100-20251215-195000.txt`)
  - `dxcluster/gridstore.(*Store).Get` cum `~8.50s` (`data/diagnostics/pprof-cpu-list-gridstore-Get-20251215-195000.txt`)
  - `main.(*gridCache).shouldUpdate` cum `~7.96s`, where the cache-miss DB check dominates (`store.Get` is `~7.69s` inside it; see `data/diagnostics/pprof-cpu-list-gridCache-shouldUpdate-20251215-195000.txt`)

Inside the output loop (`data/diagnostics/pprof-cpu-list-processOutputSpots-20251215-195000.txt`), the biggest cumulative stalls are:
- `gridUpdate(s.DXCall, dxGrid)` cum `~3.46s`
- `gridUpdate(s.DECall, deGrid)` cum `~4.51s`
- `applyLicenseGate(s, ...)` cum `~0.75s`
- `maybeApplyCallCorrectionWithLogger(...)` cum `~0.66s`

Interpretation:
- The output pipeline’s biggest synchronous cost is **grid persistence/update**, specifically the **SQLite read on cache miss** inside `gridCache.shouldUpdate`.
- GC work is visible in the CPU top (`runtime.scanobject`, `runtime.gcDrain`, etc.); heap alloc profiles show the cause (next section).

#### Heap in-use (end)
- End heap in-use: `~82.65MB` (`data/diagnostics/pprof-heap-inuse-20251215-195000.txt`).
- Biggest live owners:
  - `dxcluster/spot.NewSpot`: `~36MB` in-use (expected: ring buffer + downstream caches keep spots alive)
  - Dedup + secondary dedup caches: a few MB
  - CTY database structures: `~10.8MB` cumulative in loader path
- Heap growth over the 15m window is modest (`data/diagnostics/pprof-heap-diff-inuse-20251215-195000.txt`), not consistent with a leak.

#### Heap allocations (alloc_space): primary driver is call correction cache initialization
- Total allocations since process start at capture time: `~8.0GB` (`data/diagnostics/pprof-heap-alloc-20251215-195000.txt`).
- Largest allocation source by far:
  - `dxcluster/spot.(*distanceCache).configure`: `~5.39GB` (**~69% of all allocations**)
  - Root cause is a per-decision `make(map[string]distanceCacheEntry, max)` (line 162 in `spot/correction.go`; see `data/diagnostics/pprof-alloc-list-distanceCache-configure-20251215-195000.txt`).

Second-tier allocation sources once the above is fixed:
- MQTT route matching (`paho.mqtt`): `strings.Split` cum `~630MB` (~8% of allocations) (`data/diagnostics/pprof-heap-alloc-cum-20251215-195000.txt`).
- Spot creation + broadcast cloning: `spot.NewSpot ~270MB`, `cloneSpotForBroadcast ~130MB`.

### What appears improved vs the older 2025-12-08 profiles
- CTY lookup is no longer a top CPU consumer in the new 15m profile (it shows up as small cache operations instead of dominating CPU).
- FCC ULS checks are no longer prominent in CPU top/cum; caching in `uls/license_check.go` likely pushed this off the hot set for this workload.

### Updated remaining recommendations (prioritized)

#### P0: Fix call correction distance-cache allocation blowup (largest win; also lowers GC/latency jitter)
Evidence:
- `spot.(*distanceCache).configure` allocates ~`5.39GB` (~`69%`) of all allocation traffic (`data/diagnostics/pprof-heap-alloc-20251215-195000.txt`).
- This is driven by preallocating a large map (`distance_cache_size: 8192`) for a cache that is currently **constructed per correction decision** (`spot/correction.go` creates `localCache := &distanceCache{}` and calls `configure(...)`).

Recommendations:
1. Immediate mitigation (config-only): reduce `call_correction.distance_cache_size` drastically (e.g., `256` or `512`) while keeping correctness. This should cut allocation traffic by an order of magnitude without breaking behavior (cache is per-decision; it does not need thousands of slots).
2. Proper fix (code): avoid per-decision large-map preallocation:
   - Lazy allocate (`entries=nil` until first `put`), or allocate with a small cap derived from `len(callStats)` instead of `max`, and/or
   - Reuse cache storage via `sync.Pool` to avoid repeated `make(map, 8192)` churn.

Estimated impact:
- Allocation traffic reduction: up to **~65–70%** (based on the observed alloc profile share).
- CPU: lower GC work (`runtime.scanobject`, `runtime.gcDrain`) → **material reduction in CPU and tail-latency variance** under real high-throughput workloads.

#### P0: Reduce grid update stalls in the output pipeline (DB reads on cache miss)
Evidence:
- `gridUpdate(...)` calls account for `~8.0s` cumulative time inside `processOutputSpots` in this run (`data/diagnostics/pprof-cpu-list-processOutputSpots-20251215-195000.txt`).
- `main.(*gridCache).shouldUpdate` spends `~7.69s` in `store.Get` on cache miss (`data/diagnostics/pprof-cpu-list-gridCache-shouldUpdate-20251215-195000.txt`).
- `gridstore.(*Store).Get` itself is almost entirely `database/sql`/sqlite time (`data/diagnostics/pprof-cpu-list-gridstore-Get-20251215-195000.txt`).

Recommendations:
1. Immediate mitigation (config-only): raise `grid_cache_size` significantly (e.g., `100000`, or set it to `0` to use the code’s 100k default) to reduce cache-miss DB checks in `shouldUpdate`.
2. Proper fix (code): do not synchronously query SQLite in the output goroutine just to avoid redundant writes. Options:
   - Move the DB “already has same grid?” check into the async writer goroutine.
   - Or accept redundant updates and rely on batching (and/or SQL conditional updates) to make writes cheap.
   - Add explicit metrics: count `shouldUpdate` fast hits vs DB checks and time spent in DB, so this can be validated under real load.

Estimated impact:
- CPU: this is ~`14–15%` of CPU samples in this run; could be a **high-leverage win** under heavy spot volume.
- Latency: removes a **synchronous DB stall** from the single-threaded output loop → improves tail latency and reduces pipeline backpressure risk.

##### A/B validation (15m headless): `grid_db_check_on_miss` toggle (implemented)

We implemented an A/B toggle to *skip the synchronous SQLite read on cache miss* inside `gridCache.shouldUpdate` (the “DB-check-on-miss” path), while keeping the original behavior available for quick rollback-by-restart.

Implementation notes:
- Config: `grid_db_check_on_miss` (default true); env override: `DXC_GRID_DB_CHECK_ON_MISS` (wins).
- When disabled, `startGridWriter` passes `store=nil` into `cache.shouldUpdate(...)`, so the cache-miss path cannot call `store.Get`.
- Goal: reduce spot pipeline tail latency by removing a blocking DB point read from the hot loop.
- Tradeoff: more redundant grid upserts on cache miss (extra writes/CPU in the async writer).

Artifacts (so we don’t need to re-run this analysis):
- Binary: `gocluster_profile_ab.exe`
- Run A (DB-check ON):
  - Session: `ab-dbcheckon-20251215-222711`
  - CPU: `data/diagnostics/cpu-15m-ab-dbcheckon-20251215-222711.pprof`
  - Heap start/end (GC forced): `data/diagnostics/heap-start-ab-dbcheckon-20251215-222711.pprof`, `data/diagnostics/heap-end-ab-dbcheckon-20251215-222711.pprof`
  - Logs: `data/diagnostics/run-ab-dbcheckon-20251215-222711.out.log`, `data/diagnostics/run-ab-dbcheckon-20251215-222711.err.log`
  - Derived text: `data/diagnostics/pprof-cpu-top80-ab-dbcheckon-20251215-222711.txt`, `data/diagnostics/pprof-cpu-cum80-ab-dbcheckon-20251215-222711.txt`, `data/diagnostics/pprof-heap-inuse-ab-dbcheckon-20251215-222711.txt`, `data/diagnostics/pprof-heap-alloc-ab-dbcheckon-20251215-222711.txt`
- Run B (DB-check OFF):
  - Session: `ab-dbcheckoff-20251215-224317`
  - CPU: `data/diagnostics/cpu-15m-ab-dbcheckoff-20251215-224317.pprof`
  - Heap start/end (GC forced): `data/diagnostics/heap-start-ab-dbcheckoff-20251215-224317.pprof`, `data/diagnostics/heap-end-ab-dbcheckoff-20251215-224317.pprof`
  - Logs: `data/diagnostics/run-ab-dbcheckoff-20251215-224317.out.log`, `data/diagnostics/run-ab-dbcheckoff-20251215-224317.err.log`
  - Derived text: `data/diagnostics/pprof-cpu-top80-ab-dbcheckoff-20251215-224317.txt`, `data/diagnostics/pprof-cpu-cum80-ab-dbcheckoff-20251215-224317.txt`, `data/diagnostics/pprof-heap-inuse-ab-dbcheckoff-20251215-224317.txt`, `data/diagnostics/pprof-heap-alloc-ab-dbcheckoff-20251215-224317.txt`
- CPU/heap deltas:
  - CPU: `data/diagnostics/pprof-cpu-delta-top80-ab-dbcheckoff-20251215-224317--ab-dbcheckon-20251215-222711.txt`
  - Heap: `data/diagnostics/pprof-heap-delta-inuse-ab-dbcheckoff-20251215-224317--ab-dbcheckon-20251215-222711.txt`, `data/diagnostics/pprof-heap-delta-alloc-ab-dbcheckoff-20251215-224317--ab-dbcheckon-20251215-222711.txt`

Run configuration (kept consistent across A/B):
- Headless: `ui.mode=headless`
- `call_correction.distance_cache_size=256`
- `grid_cache_size=10240`
- Diag server: `DXC_PPROF_ADDR=127.0.0.1:6061`

Workload at 15 minutes (from stdout logs; live feeds differ so normalize when comparing):
- Run A totals @00:15: RBN `1086`, PSKReporter `21609` → `22695` spots
- Run B totals @00:15: RBN `1112`, PSKReporter `17634` → `18746` spots

CPU results (15m):
- Total CPU samples:
  - Run A: `37.64s` / `900s` → `4.18%` of one core
  - Run B: `39.78s` / `900s` → `4.42%` of one core
- Approx CPU per spot (rough normalization): `1.66ms` (A) → `2.12ms` (B), ~`+28%`
- Grid miss-check hot-path impact (directly attributable to the toggle):
  - `main.(*gridCache).shouldUpdate` cum: `0.75s` (A) → `0.12s` (B) (Δ `-0.63s`)
  - `dxcluster/gridstore.(*Store).Get` cum: `1.03s` (A) → `0.48s` (B) (Δ `-0.55s`)
    - In Run A, `~0.58s` of `store.Get` is called from `shouldUpdate`; the remaining `~0.45s` is `lookupFn` DB fallback and remains even when DB-check-on-miss is off.
- Write amplification when DB-check-on-miss is disabled (work shifts to async writer):
  - `dxcluster/gridstore.(*Store).UpsertBatch` cum: `0.09s` (A) → `1.37s` (B) (Δ `+1.28s`)
  - SQLite exec core (`modernc.org/sqlite/lib._sqlite3VdbeExec`) cum: `2.35s` (A) → `2.71s` (B) (Δ `+0.36s`)

Grid update behavior (from `Grid database:` line):
- Cache hit rate: `85.9%` (A) vs `85.6%` (B) (similar)
- Updates since start: `401` (A) vs `7,348` (B) (~`18×` more)
  - Updates per spot: `1.77%` (A) vs `39.2%` (B)

Heap results (GC forced heap profiles):
- In-use heap total:
  - Start: `32.10MB` (A) vs `31.03MB` (B)
  - End: `59.45MB` (A) vs `74.04MB` (B)
- End heap is `+14.6MB` higher with DB-check OFF, dominated by `dxcluster/spot.NewSpot` retention (traffic/occupancy differences). Grid-cache-related allocations are small by comparison.

Interpretation:
- The toggle does what we wanted: it removes the cache-miss SQLite read from `shouldUpdate` (hot path).
- In this 15m A/B, overall CPU did **not** drop (and CPU/spot increased) because redundant grid writes increased dramatically; however, those writes are done by the async writer and are much less likely to add tail latency to `processOutputSpots` than synchronous reads.
- If the objective is primarily tail-latency, keeping `grid_db_check_on_miss=false` can still be the right choice on SSDs, but it is a deliberate “more background work for less blocking” trade.

Recommended rollout posture:
- Keep A/B toggle and measure tail latency (separately from CPU). Use `grid_db_check_on_miss=false` only if the latency win is real and worth the write amplification.
- If we keep DB-check OFF long-term, consider a follow-up optimization to reduce redundant writes without reintroducing a synchronous read (e.g., larger/cheaper cache, “insert-or-ignore” semantics for known rows, or a compact membership filter).

#### P1: MQTT routing allocation overhead (next biggest allocator after call correction fix)
Evidence:
- `strings.Split` via `paho.mqtt` routing is `~630MB` (~8%) of allocations (`data/diagnostics/pprof-heap-alloc-cum-20251215-195000.txt`).

Recommendations:
- Reduce the number of topic subscriptions/routes if possible (fewer topic patterns → less route matching work).
- If this remains hot after P0, consider a single wildcard subscription with custom dispatch or an MQTT client/router that avoids per-message `strings.Split` churn.

Estimated impact:
- Allocation reduction: **~5–10%** once the call correction allocator is fixed.

#### P2: Broadcast clone allocations (minor, but easy win if behavior allows)
Evidence:
- `cloneSpotForBroadcast` allocates `~130MB` (~1.6%) (`data/diagnostics/pprof-heap-alloc-20251215-195000.txt`).

Recommendations:
- If acceptable, keep SSIDs on broadcast to avoid cloning.
- Otherwise, optimize cloning/formatting to avoid allocating a full duplicate spot for SSID collapse (code change).

Estimated impact:
- Allocation reduction: **~1–2%** (small), but also reduces GC churn.

#### Profiling setup gaps (optional future work)
- `/debug/pprof/mutex` and `/debug/pprof/block` show `0` in this run (these require `runtime.SetMutexProfileFraction` / `runtime.SetBlockProfileRate` in code). If lock contention becomes a concern under load, add env-configurable toggles for these.

---

## 0) Scope, Environment, and Method

### Scope
- Repo: `c:\src\gocluster`
- Goal: reduce *spot processing latency* (ingest→broadcast) and *overall CPU utilization*.
- Constraint: no code changes in this work product; this is analysis + proposal only.

### Environment observed during analysis
- OS: Windows (PowerShell)
- Go toolchain: `go version go1.25.4 windows/amd64`
- Repo contains existing performance notes and pprof captures under `data/diagnostics/`.

### Method
1. Read repo docs (`README.md`, `OPTIMIZATION.md`, `CPU_MEMORY_OPTIMIZATION_ANALYSIS.md`, `PERFORMANCE_RELIABILITY_REVIEW.md`) to map architecture.
2. Identify “hot path” candidates by code structure (per-spot loops, per-client fanout, cache/DB lookups).
3. Use *existing* CPU profiles in `data/diagnostics/*.pprof` to ground the analysis in evidence:
   - `data/diagnostics/cpu-20251208-112420.pprof` (baseline 120s)
   - `data/diagnostics/cpu-pskopt-20251208-130954.pprof` (120s)
   - `data/diagnostics/cpu-15m-20251208-135904.pprof` (900s)
4. For each hotspot, perform a line-by-line review of the relevant function(s) and evaluate CPU efficiency, allocation behavior, lock contention risk, and blocking I/O risk.
5. Produce a prioritized set of optimizations with estimated impact.

Notes:
- Profile build IDs indicate the exact `gocluster.exe` used for the capture. Source code line numbers can drift if the repo changed after the executable was built. This writeup therefore references both:
  - Evidence from pprof listings (“this line was hot in the capture”), and
  - The current source functions and their structure (“this is how it works now”).

---

## 1) Repo Architecture and Spot Dataflow (What the Code Does)

### Topology (high level)
1. **Ingest**
   - RBN telnet feeds (CW/RTTY + digital) in `rbn/client.go`
   - PSKReporter MQTT in `pskreporter/client.go`
   - Optional human/upstream telnet feed in `rbn/client.go` with `minimalParse` support
2. **Normalization + enrichment**
   - Callsign normalization and validation: `spot/callsign.go`
   - CTY prefix metadata lookup: `cty/parser.go` (cache-backed longest-prefix lookup)
   - FCC ULS license gating: `uls/license_check.go` + final gate in `main.go` (`applyLicenseGate`)
3. **Protections**
   - Primary dedup: `dedup/deduplicator.go` (sharded cache keyed by `Spot.Hash32()`)
   - Call correction (CW/RTTY/SSB): `spot/correction.go` + `main.go:maybeApplyCallCorrectionWithLogger`
   - Harmonic suppression: `spot/harmonics.go`
   - Frequency averaging (CW/RTTY): `spot/frequency_averager.go`
   - Secondary broadcast-only dedup: `dedup/secondary.go`
4. **Persistence**
   - Ring buffer for recent spots: `buffer/ringbuffer.go` (atomic pointers)
   - Grid/known calls DB (SQLite): `gridstore/store.go` with writer/cache in `main.go`
5. **Output**
   - Telnet server + broadcast worker pool: `telnet/server.go`
   - Spot formatting for DX cluster lines: `spot/spot.go` (`Spot.FormatDXCluster`, cached via `sync.Once`)
6. **Local UI / stats**
   - Optional console UIs: ANSI or Tview (`main.go` + dashboard files)
   - Stats printed periodically (`stats/` + `main.go` formatting helpers)

### Key pipeline (runtime order)
- Ingest clients parse payload/line → create `*spot.Spot` with metadata → send into `dedup.Deduplicator` input channel.
- Deduplicator computes `Spot.Hash32()` → sharded map check → emits to output channel.
- `main.processOutputSpots` reads output channel and runs:
  - normalization (`EnsureNormalized`), grid backfill (optional), call correction (optional), harmonic suppression (optional), frequency averaging (optional), confidence glyph defaults, final CTY/license gate, stats accounting, grid persistence updates, ring buffer append, secondary dedupe, telnet broadcast.

### `config/mode_allocations.yaml` (your active tab)
- Loaded/used by `rbn/client.go` for “minimal/human” telnet parsing when a spot line lacks an explicit mode.
- It provides coarse Region 1 CW vs voice splits by band. This is not on the CPU hot path in the profiled workloads; it matters for correctness of inferred mode, not throughput.

---

## 2) Evidence: Existing CPU Profiles (pprof) and What They Imply

Below are the salient pprof outputs used to identify bottlenecks.

### 2.1 Baseline 120s profile: `data/diagnostics/cpu-20251208-112420.pprof`

Command used:
- `go tool pprof -top -nodecount=30 .\\gocluster.exe .\\data\\diagnostics\\cpu-20251208-112420.pprof`

Output (abridged):
```
Total samples = 19.15s (15.96%)
Showing top 30 nodes out of 266
      flat  flat%   sum%        cum   cum%
     4.35s 22.72% 22.72%      4.35s 22.72%  memeqbody
        3s 15.67% 38.38%      3.07s 16.03%  runtime.cgocall
     1.16s  6.06% 44.44%      4.20s 21.93%  runtime.scanobject
     0.83s  4.33% 48.77%      1.21s  6.32%  runtime.findObject
     0.60s  3.13% 58.80%      5.35s 27.94%  dxcluster/cty.(*CTYDatabase).lookupCallsignNoCache
     0.12s  0.63% 71.91%      4.74s 24.75%  strings.HasPrefix (inline)
```

Cumulative view:
- `go tool pprof -top -cum -nodecount=30 ...`
```
      flat  flat%   sum%        cum   cum%
      7.45s 38.90%  dxcluster/pskreporter.(*Client).workerLoop
      7.39s 38.59%  dxcluster/pskreporter.(*Client).handlePayload
      7.23s 37.75%  dxcluster/pskreporter.(*Client).convertToSpot
      5.46s 28.51%  dxcluster/cty.(*CTYDatabase).LookupCallsign
      5.35s 27.94%  dxcluster/cty.(*CTYDatabase).lookupCallsignNoCache
      3.37s 17.60%  main.processOutputSpots
      2.36s 12.32%  dxcluster/uls.IsLicensedUS
      2.13s 11.12%  dxcluster/gridstore.(*Store).Get
```

Implications:
- CTY lookups dominate CPU: `lookupCallsignNoCache` + `strings.HasPrefix` + `memeqbody`.
- SQLite queries are meaningful: FCC ULS (`uls.IsLicensedUS`) + gridstore lookups.
- Output stage (`processOutputSpots`) is a major cumulative hotspot (single goroutine doing many steps).
- PSKReporter ingestion stack is heavy *cumulatively* (not necessarily flat CPU inside PSK; it calls into CTY/DB/etc).

### 2.2 120s “pskopt” profile: `data/diagnostics/cpu-pskopt-20251208-130954.pprof`

Command used:
- `go tool pprof -top -cum -nodecount=60 -nodefraction=0 .\\gocluster.exe .\\data\\diagnostics\\cpu-pskopt-20251208-130954.pprof`

Hot cumulative nodes (abridged):
```
8.37s 45.81%  dxcluster/pskreporter.(*Client).workerLoop
8.31s 45.48%  dxcluster/pskreporter.(*Client).handlePayload
8.13s 44.50%  dxcluster/pskreporter.(*Client).convertToSpot
5.80s 31.75%  dxcluster/cty.(*CTYDatabase).LookupCallsign
5.64s 30.87%  dxcluster/cty.(*CTYDatabase).lookupCallsignNoCache
3.58s 19.59%  main.processOutputSpots
2.79s 15.27%  dxcluster/uls.IsLicensedUS
2.34s 12.81%  dxcluster/gridstore.(*Store).Get
2.24s 12.26%  main.(*gridCache).shouldUpdate
```

Key line-level evidence from `pprof -list`:

1) CTY scan loop:
- `go tool pprof -list=lookupCallsignNoCache ...`
```
580ms flat, 5.64s cum (30.87% of Total)
163: for _, key := range db.Keys {
167: if strings.HasPrefix(cs, key) {   <-- 5.05s attributed here
```

2) Gridstore DB read:
- `go tool pprof -list='\\(\\*Store\\)\\.Get' ...`
```
261: err := s.db.QueryRow(...).Scan(...)  <-- 1.90s QueryRow, 430ms Scan
```

3) FCC ULS DB query:
- `go tool pprof -list=IsLicensedUS ...`
```
72: err := db.QueryRow("SELECT 1 FROM AM ...", canonical).Scan(&dummy)  <-- 2.77s
```

4) Grid cache “miss → DB read” policy:
- `go tool pprof -list=shouldUpdate ...`
```
174: if store != nil { if rec, err := store.Get(call); ... }  <-- 2.19s
```

Implications:
- CTY algorithmic cost is the clearest CPU lever.
- SQLite point lookups (grid + ULS) are very expensive relative to in-memory operations and also add latency variability.
- Grid persistence logic is unintentionally expensive on cache misses because it reads SQLite to avoid redundant writes.

### 2.3 15-minute profile: `data/diagnostics/cpu-15m-20251208-135904.pprof`

Command used:
- `go tool pprof -top -nodecount=40 .\\gocluster.exe .\\data\\diagnostics\\cpu-15m-20251208-135904.pprof`

Output (abridged):
```
Duration: 900.16s, Total samples = 276.98s (30.77%)
flat 214.06s 77.28%  runtime.cgocall
cum 109.10s 39.39%   github.com/gdamore/tcell/v2.(*cScreen).draw
cum 103.14s 37.24%   syscall.WriteConsole
```

Implications:
- If `ui.mode` is not `headless` (especially `tview`), console I/O can dominate CPU, masking all spot-pipeline optimizations.
- For production CPU + latency evaluation, measure with `ui.mode: headless` (or at least no high-frequency redraw).

---

## 3) Hot Path Functions (Primary) and Line-by-Line CPU Review

This section focuses on the functions implicated by pprof and/or obviously in the per-spot path.

### 3.1 CTY lookup: `cty/parser.go` `(*CTYDatabase).lookupCallsignNoCache`

Why hot:
- pprof attributes ~30% cumulative CPU to this routine, and ~5s in a 120s sample directly to `strings.HasPrefix` inside the scan loop.

Key code path (current structure):
1. `if info, ok := db.Data[cs]; ok { return clonePrefix(info), true }`
   - Map lookup is O(1) average (fast).
   - `clonePrefix(info)` allocates a new `PrefixInfo` on the heap each hit (per lookup allocation).
2. `for _, key := range db.Keys { ... if strings.HasPrefix(cs, key) { ... } }`
   - `db.Keys` is sorted longest-first. That’s correct for longest-prefix match, but it’s still an O(#keys) scan on misses.
   - `strings.HasPrefix` for each key results in repeated byte comparisons; pprof surfaces that as `memeqbody` and `HasPrefix` cumulative time.

CPU efficiency assessment:
- Algorithmically expensive for misses and for calls where the matching prefix is short (you scan many keys before finding a match).
- Allocation-heavy even for hits due to `clonePrefix`.
- Cache exists (`cacheGet`/`cacheStore`), but high-cardinality inputs (lots of unique calls) will still trigger the scan and/or allocate many cache entries.

Latency impact:
- This runs inside the per-spot path for multiple calls (DX + DE) and can run multiple times per spot across pipeline stages if not carefully de-duplicated.

### 3.2 Output stage: `main.go` `processOutputSpots`

Why hot:
- ~18–20% cumulative in the 120s profiles.
- It is single-threaded (one goroutine reading the dedup output channel).
- It performs blocking operations (SQLite reads) in-line.

CPU/latency relevant blocks:
1. `s.EnsureNormalized()`
   - Good: centralizes normalization to avoid repeated `ToUpper/TrimSpace`.
2. Grid backfill:
   - `if strings.TrimSpace(s.DXMetadata.Grid) == "" { gridLookup(s.DXCall) }`
   - `gridLookup` can call SQLite on cache miss.
   - This is a per-spot synchronous DB read on a missing optional field; high latency variability and can stall the entire output pipeline.
3. Call correction / harmonic / averaging:
   - Optional; can be CPU-heavy depending on config and traffic.
4. Final CTY/license gate: `applyLicenseGate(s, ctyDB, unlicensedReporter)`
   - Performs CTY lookups for DX and DE (and license-normalized variants), then may call FCC ULS DB.
5. Stats + grid updates:
   - Grid updates call `gridUpdate` which calls `gridCache.shouldUpdate`, which (on miss) calls SQLite (`store.Get`) synchronously.
6. Telnet broadcast:
   - `telnet.BroadcastSpot` is non-blocking and drops on queue full (good for backpressure).
   - Optional “collapse SSID” clones the spot for broadcast (allocates).

CPU efficiency assessment:
- Structure is readable and correct but mixes:
  - CPU-heavy steps,
  - Blocking DB I/O,
  - And fanout/broadcast orchestration
  in one hot loop.
- The biggest latency lever is removing blocking SQLite reads from this loop (or pushing them to async warmers / caches).

### 3.3 Grid cache “shouldUpdate”: `main.go` `(*gridCache).shouldUpdate`

Why hot:
- ~12% cumulative in a 120s profile, mostly due to the DB read on cache miss.

Key behavior:
1. Lock cache mutex, check entry, update in-memory LRU.
   - Fine; O(1) typical.
2. On miss, it queries SQLite to avoid redundant writes:
   - `store.Get(call)` dominates runtime.

CPU efficiency assessment:
- The DB “read to avoid write” is the wrong tradeoff if the read is expensive and the write is already batched.
- Current cache size in `config.yaml` is small (`grid_cache_size: 3000`), which guarantees frequent misses under high-cardinality streams (e.g., PSKReporter).

Latency impact:
- This call happens in the hot output loop (grid updates), so each cache miss can stall spot processing.

### 3.4 Gridstore “Get”: `gridstore/store.go` `(*Store).Get`

Why hot:
- ~11–13% cumulative in 120s profiles.
- Called by:
  - grid backfill (`gridLookup` inside `processOutputSpots`)
  - grid cache miss DB check (`gridCache.shouldUpdate`)

CPU efficiency assessment:
- The SQL itself is simple; overhead is in `database/sql` + sqlite driver + I/O.
- Avoiding the call (by increasing cache hit rate, removing the “read to avoid write”, and removing synchronous backfill) is better than micro-optimizing the SQL.

### 3.5 FCC ULS license check: `uls/license_check.go` `IsLicensedUS`

Why hot:
- ~12–15% cumulative in 120s profiles, with the point query line dominating.

Key behavior:
1. Normalize call (`NormalizeForLicense`) – string ops; acceptable.
2. Cache via `sync.Map` (process lifetime, unbounded growth).
3. On cache miss: query SQLite:
   - `SELECT 1 FROM AM WHERE call_sign = ? LIMIT 1;`
   - Has retry loop for SQLITE_BUSY.

CPU efficiency assessment:
- SQLite query cost dominates and adds latency jitter.
- Cache avoids repeated lookups for repeated calls; but for high-cardinality traffic the miss rate can still be significant.
- Cache is unbounded; long-running processes may accumulate many unique calls.

### 3.6 PSKReporter ingest: `pskreporter/client.go`

Why hot (cumulative):
- `workerLoop/handlePayload/convertToSpot` are ~39–46% cumulative in the 120s captures, largely because they drive CTY and license lookups and build spots.

Notable CPU/alloc aspects:
1. `messageHandler`: allocates and copies `msg.Payload()` into a new `[]byte` each message.
   - This is pure allocation/copy overhead; good candidate for pooling if PSK volume is high.
2. `handlePayload`: JSON unmarshal per message.
   - Can be heavy under high message rate; but pprof evidence suggests CTY/DB dominate overall more than JSON itself in the sampled runs.
3. `convertToSpot`: validates fields, normalizes, validates calls, CTY lookups for DX and DE, license check for DE (spotter).
   - If invalid payloads are frequent and logged, logging can become significant (in a pprof listing, one invalid-timestamp log line was attributed multiple seconds in the sample).

### 3.7 RBN ingest: `rbn/client.go` `(*Client).parseSpot` and helpers

Not seen in the provided pprof hot list (likely because the profiled workload emphasized PSKReporter), but it is inherently a hot loop under RBN load.

CPU/alloc review (current code structure):
1. `whitespaceRE.ReplaceAllString(line, " ")` then `strings.Fields(normalized)`
   - Regex replace is expensive relative to simple scanning; currently done unconditionally for every line.
2. `splitSpotterToken` may allocate a new slice to insert frequency fragments.
3. `findFrequencyField` scans tokens, parses float repeatedly until it finds plausible frequency.
4. Uses `strings.Join` to build comment and `fmt.Sprintf` for CW WPM formatting.
5. CTY lookups for DX and DE; license check for US DE; build spot; enqueue.

CPU efficiency assessment:
- Under RBN bursts, avoidable allocations (regex, joins, Sprintf) will matter.
- However, based on existing pprof evidence, the *largest* shared CPU levers are still CTY and SQLite, not per-line string mechanics (unless you’re RBN-heavy and PSK-light).

### 3.8 Telnet broadcasting: `telnet/server.go`

Not currently dominating CPU in the headless profiles, but it can become a bottleneck if:
- client count is high, or
- per-client filtering is expensive, or
- broadcast batching interval is too low (more syscalls).

Key behaviors:
- Non-blocking broadcast queue + per-worker queues + per-client channels. Drops are logged with rate-limiting.
- Worker batching controlled by `telnet.broadcast_batch_interval_ms` (configurable; trades latency for CPU).

---

## 4) The “CPU Root Causes” and “Latency Root Causes” (From Evidence)

### CPU root causes (highest confidence)
1. CTY lookup algorithm: full scan of `db.Keys` with `strings.HasPrefix` for misses (`cty/parser.go`).
2. SQLite point queries in the spot pipeline:
   - FCC ULS license checks (`uls.IsLicensedUS`)
   - Gridstore reads (`gridstore.Store.Get`), often triggered by cache miss policies (`gridCache.shouldUpdate`) and optional grid backfill (`processOutputSpots`).
3. Console UI overhead when enabled (`ui.mode: tview`): can dominate CPU via syscalls.

### Latency root causes (highest confidence)
1. Blocking SQLite reads inside a single-threaded output pipeline (`processOutputSpots`):
   - grid backfill lookups and “read-to-avoid-write” in grid updates.
2. Broadcast micro-batching adds up to `broadcast_batch_interval_ms` of additional delay (plus queueing).
3. When the output loop is slowed, it backlogs the dedup output channel; downstream latency increases and upstream drops may occur (by design).

---

## 5) Recommendations (Prioritized) With Estimated Impact

Impact estimates are based on the observed share of CPU in pprof. “Overall CPU” estimates assume other costs remain roughly constant; real gains should be validated under controlled load.

### P0 (highest ROI, lowest complexity / risk)

#### P0.1 Run headless in production profiling and most deployments
- Action: set `ui.mode: headless` (config-only).
- Evidence: 15-minute profile dominated by console (`runtime.cgocall` ~77% flat).
- Expected impact:
  - If currently `tview`/ANSI redraw is active: **very large CPU reduction** (often tens of percent to >50% depending on refresh and traffic) and less jitter.
  - If already headless: no change.
- Latency: can improve due to fewer syscalls and less contention.

#### P0.2 Replace CTY longest-prefix scan with an O(len(call)) lookup (prefix-chop or trie)
- Where: `cty/parser.go` `lookupCallsignNoCache` (currently scan + `HasPrefix`).
- Evidence: ~30–31% cumulative CPU with ~5s attributed to `strings.HasPrefix` line in a 120s sample.
- Recommendation: implement one of:
  - Prefix-chop: check exact match, then progressively shorten `cs` and probe `db.Data[prefix]`.
  - Trie / radix structure built once at startup.
- Expected impact:
  - Reduce CTY CPU cost by **~80–95%** for misses.
  - Overall CPU reduction: roughly `CTY_share * reduction`, i.e. **~15% to ~30%** overall CPU in headless profiles where CTY is ~30% cumulative.
- Latency: materially reduces per-spot compute time and queue buildup.

#### P0.3 Stop doing synchronous SQLite reads to “avoid redundant grid writes”
- Where: `main.go` `gridCache.shouldUpdate` (the miss path calls `store.Get`).
- Evidence: `gridCache.shouldUpdate` ~12% cumulative, with ~2.19s attributed to `store.Get(call)` in 120s sample.
- Recommendation options:
  1) Config-first: drastically increase `grid_cache_size` to reduce miss frequency (see P0.4).
  2) Architectural: remove the DB read on cache miss; accept occasional redundant writes since writes are batched anyway.
- Expected impact:
  - If DB read is removed and cache remains moderate: **~5% to ~12% overall CPU** reduction (workload dependent) + reduced latency jitter.
  - If only cache size is increased: the gain depends on achieved hit rate; can still be significant.

#### P0.4 Increase `grid_cache_size` (config-only quick win)
- Current config: `grid_cache_size: 3000` (very small for PSK-scale cardinality).
- Expected impact: reduces calls to `gridstore.Get`, which is ~11–13% cumulative in headless pprof.
- Suggested ranges:
  - 50k–200k for PSK-heavy workloads (memory tradeoff).
- Expected impact: **~5% to ~12% overall CPU** depending on miss rate; also improves output-stage latency.

### P1 (high ROI, moderate complexity)

#### P1.1 Remove synchronous grid backfill from the output hot loop (or make it cache-only)
- Where: `main.go` `processOutputSpots` gridLookup for DX/DE grids when missing.
- Evidence: output loop is hot; gridstore reads are expensive; grid backfill is optional metadata.
- Expected impact: **up to ~5–10% overall CPU** (depends on how often grids are missing) + better tail latency.

#### P1.2 Reduce redundant CTY lookups across stages (ingest vs final gate)
- Where: `pskreporter/client.go` + `rbn/client.go` do CTY lookups; output `applyLicenseGate` does CTY again (and also on license-normalized variants).
- Expected impact: **~5–15% overall CPU** in workloads where the same call is looked up multiple times per spot.
- Latency: fewer CTY calls per spot reduces queueing.

#### P1.3 Reduce SQLite overhead for license and grid queries (prepared statements + tuned pool)
- Where:
  - `uls/license_check.go` per-call point query
  - `gridstore/store.go` Get query
- Expected impact:
  - Prepared statements + stable connection pool can reduce overhead by **~2–6% overall CPU** (depends on how much time is in prepare/open vs execution).
  - More important: reduces tail latency and variability.

### P2 (high upside but higher complexity / operational considerations)

#### P2.1 Replace FCC ULS per-call SQLite checks with an in-memory membership structure
- Where: `uls.IsLicensedUS`
- Evidence: ~12–15% cumulative in headless 120s profiles.
- Expected impact:
  - If successful: eliminate most per-spot license DB queries → **~8–15% overall CPU** reduction + major latency improvement.
- Tradeoffs:
  - Memory footprint, startup time, and refresh strategy.
  - Must preserve correctness; probabilistic filters (Bloom) are risky unless designed carefully (false positives would incorrectly allow unlicensed calls).

#### P2.2 Parallelize/split the output pipeline (reduce queueing latency)
- Where: `main.processOutputSpots` (single goroutine)
- Goal: reduce end-to-end latency under load by avoiding one serial bottleneck; isolate blocking IO.
- Expected impact:
  - **Large tail-latency improvement** when CPU is saturated or DB stalls occur.
  - CPU savings: not guaranteed; may increase due to overhead.
- Risk: complexity, ordering semantics, and debugging complexity.

#### P2.3 PSKReporter micro-optimizations (after CTY/DB fixes)
- Pool `[]byte` payload buffers in `messageHandler` (remove alloc+copy per message).
- Reduce JSON unmarshal cost by streaming/partial decoding.
- Rate-limit noisy logs on invalid payloads (if observed).
- Expected impact: **~2–10% overall CPU** depending on PSK message rate and remaining bottlenecks.

### P3 (nice-to-have / conditional)
- Shard or redesign callsign normalization cache if lock contention appears under concurrency (`spot/callsign.go` CallCache uses a single mutex).
- Optimize RBN parsing string mechanics (regex bypass, fewer joins/Sprintf) if RBN ingestion dominates your traffic profile.

---

## 6) “Latency vs CPU” Tradeoffs (Explicit Knobs)

These are the highest-leverage knobs that directly trade latency for CPU:

1) Telnet broadcast batching (`telnet.broadcast_batch_interval_ms`)
- Higher interval → fewer writes/syscalls → lower CPU, but adds latency (0..interval).
- Lower/zero interval → lower latency, but higher CPU and more contention at high client counts.

2) Output-stage synchronous lookups (grid backfill, grid cache DB checks, CTY/license gates)
- Making them async/cache-only reduces latency variance but may reduce enrichment fidelity or increase DB writes.

3) UI refresh (`ui.mode`, `ui.refresh_ms`)
- Tview/ANSI redraw can dominate CPU; headless is best for throughput/latency evaluation.

---

## 7) Measurement / Validation Plan (So We Can Prove Improvements)

### Built-in diagnostics server (already in code)
`main.go` includes `maybeStartDiagServer()` which exposes:
- `/debug/pprof/*`
- `/debug/heapdump` (writes `data/diagnostics/heap-<ts>.pprof`)

Enable via environment variables:
- `DXC_PPROF_ADDR=localhost:6061` (starts HTTP server)
- Optional: `DXC_HEAP_LOG_INTERVAL=60s` (periodic heap stats in logs)

Collect profiles:
- CPU: `curl "http://localhost:6061/debug/pprof/profile?seconds=120" -o data/diagnostics/cpu-<ts>.pprof`
- Heap: `curl "http://localhost:6061/debug/pprof/heap" -o data/diagnostics/heap-<ts>.pprof`
- Heap dump: `curl "http://localhost:6061/debug/heapdump"`

Compare:
- `go tool pprof -top/-cum -nodecount=50 gocluster.exe data/diagnostics/cpu-<ts>.pprof`

Important note:
- `cmd/run-with-diag.ps1` currently sets `HEAP_DIAG_ADDR`, but the code expects `DXC_PPROF_ADDR`. Update the script or use env vars manually when running diagnostics.

### Profiling hygiene
- Profile with `ui.mode: headless` to avoid UI dominating CPU.
- Use controlled load where possible (repeatable traffic/rate) to reduce noise.
- Record config knobs with each profile capture (especially: CTY cache cap, grid cache size, dedup windows, broadcast batching).

---

## 8) Actionable “Next Steps” Checklist (No-Code / Low-Code Order)

If the goal is immediate improvement without changing Go code:
1) Switch to `ui.mode: headless` for production and profiling.
2) Increase `grid_cache_size` substantially (and monitor hit rate if available).
3) Confirm `DXC_PPROF_ADDR` based diagnostics are enabled and capture a new controlled CPU profile.
4) Decide on latency target vs CPU target:
   - If latency target is strict: consider reducing `telnet.broadcast_batch_interval_ms`.
   - If CPU target is strict: keep batching and focus on CTY/SQLite elimination.

Then, for code work (future):
5) Replace CTY scan algorithm (largest CPU ROI).
6) Remove grid-cache DB reads on misses and/or make grid backfill cache-only.
7) Reduce/avoid per-spot SQLite license queries (prepared statements and/or in-memory set).
8) Consider splitting/parallelizing the output pipeline if tail latency remains high under load.

---

## 9) Key Files Referenced (for fast navigation)

- Architecture: `README.md`
- Existing perf notes: `OPTIMIZATION.md`, `CPU_MEMORY_OPTIMIZATION_ANALYSIS.md`, `PERFORMANCE_RELIABILITY_REVIEW.md`
- CTY: `cty/parser.go`
- Output pipeline: `main.go` (`processOutputSpots`, `applyLicenseGate`, `gridCache`, `startGridWriter`, diag server helpers)
- Gridstore: `gridstore/store.go`
- License checks: `uls/license_check.go`
- PSKReporter: `pskreporter/client.go`
- RBN: `rbn/client.go`
- Dedup: `dedup/deduplicator.go`, `dedup/secondary.go`
- Telnet broadcast: `telnet/server.go`
- Spot model + formatting/hash: `spot/spot.go`, `spot/callsign.go`
